{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Factors.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyj19/TS/blob/master/Deep_Factors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxHpi6CjWlg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "84074d97-02c9-45ad-ce79-914140381da9"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63MFgpVHUb5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB_-DYtEUfH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f2e47cbb-0e5c-4441-e723-72108243711d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAw0rtMUhZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c77f742-4407-472f-fdd9-ab8824a3fdde"
      },
      "source": [
        "root_path = 'gdrive/My Drive/FinancialTS/JPmarket_dataset.npz' \n",
        "data = np.load(root_path)\n",
        "data.files"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_ratios', 'test_ratios', 'train_volumes', 'test_volumes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzIcj78GVk8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ratios = data['train_ratios']\n",
        "test_ratios = data['test_ratios']\n",
        "train_vols = data['train_volumes']\n",
        "test_vols = data['test_volumes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyk_YlK6VrE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "av_vols = np.mean(train_vols[3], axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DKkWxWhOkne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "4c5d9cf4-e731-4701-e98d-2b9246f3de43"
      },
      "source": [
        "plt.plot(av_vols)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1b52294c18>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfzb9l0IoD6a0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC2J6fvWqn8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create covariates - currently one hot vector 64 dims but experiment with different ideas \n",
        "def create_covariate_data(input_data, freq = 64): #this is assuming that data starts at beginning of day and ends at last bin of the day \n",
        "  num_series, len_series = input_data.shape\n",
        "  days = int(len_series/freq)\n",
        "  covariate_vectors = np.zeros((num_series, len_series, freq+1))\n",
        "  \n",
        "  for n in range(num_series):\n",
        "    for d in range(days): \n",
        "      for t in range(freq): \n",
        "        one_hot = np.zeros(freq)\n",
        "        one_hot[t] = 1\n",
        "        covariate_vectors[n, d*64 + t, 0] = input_data[n, d*64 + t]\n",
        "        covariate_vectors[n, d*64 + t, 1:] = one_hot\n",
        "        \n",
        "  return covariate_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMl5hXU41NDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_volume = train_vols[0:50].reshape((50,-1))\n",
        "\n",
        "norm_train_vols = np.zeros((50, 29568))\n",
        "for i in range(50): \n",
        "  norm_train_vols[i] = train_volume[i]/np.amax(train_vols[i])\n",
        "  \n",
        "T = 64*3\n",
        "new_train_data = norm_train_vols[:,:T]\n",
        "\n",
        "covars_data = create_covariate_data(new_train_data)\n",
        "\n",
        "covars_data = torch.FloatTensor(covars_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBv4MeQbt9mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GlobalEffects(nn.Module): \n",
        "  def __init__(self, input_size, num_factors, hidden_size, batch_size = 1, output_size = 1, num_layers = 1): \n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    #how can I simplify this code and also let number of factors be a variable?\n",
        "    \n",
        "    self.lstm1 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm2 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm3 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm4 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm5 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm6 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm7 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm8 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm9 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.lstm10 = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    \n",
        "    self.w = torch.nn.Parameter(torch.zeros(batch_size, num_factors))\n",
        "    \n",
        "  def init_hidden(self): \n",
        "    return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "  def forward(self, input_data, hidden): \n",
        "    x = input_data[:,1:]\n",
        "    g1, hidden = self.lstm1(x.view(len(x), self.batch_size, -1))\n",
        "    g2, hidden = self.lstm2(x.view(len(x), self.batch_size, -1))\n",
        "    g3, hidden = self.lstm3(x.view(len(x), self.batch_size, -1))\n",
        "    g4, hidden = self.lstm4(x.view(len(x), self.batch_size, -1))\n",
        "    g5, hidden = self.lstm5(x.view(len(x), self.batch_size, -1))\n",
        "    g6, hidden = self.lstm6(x.view(len(x), self.batch_size, -1))\n",
        "    g7, hidden = self.lstm7(x.view(len(x), self.batch_size, -1))\n",
        "    g8, hidden = self.lstm8(x.view(len(x), self.batch_size, -1))\n",
        "    g9, hidden = self.lstm9(x.view(len(x), self.batch_size, -1))\n",
        "    g10, hidden = self.lstm10(x.view(len(x), self.batch_size, -1))\n",
        "\n",
        "    g = torch.cat((g1.view(-1,1), g2.view(-1,1), g3.view(-1,1), g4.view(-1,1), g5.view(-1,1), g6.view(-1,1), g7.view(-1,1), g8.view(-1,1), g9.view(-1,1), g10.view(-1,1)), dim=1)\n",
        "    #fixed_effects = torch.sum(g.view(-1, 1) * self.w)\n",
        "    #print('f', fixed_effects.shape)\n",
        "    \n",
        "    fixed_effects = torch.zeros(g.shape[0])\n",
        "    for i in range(g.shape[0]): \n",
        "      fixed_effects[i] = torch.dot(self.w.view(-1), g[i])\n",
        "\n",
        "    return fixed_effects\n",
        "    \n",
        "    #NEED TO ADD AN ATTENTION LAYER HERE \n",
        "    #also, maybe should have more hidden layers and then a dense layer to get the dims down to 1\n",
        "    #make hidden layers separate names and initialize 10 of them?\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSSlj0dlw0pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DF_RNN(nn.Module): \n",
        "  def __init__(self, input_size, hidden_size, batch_size, output_size):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.rnn = nn.RNN(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = 1)\n",
        "    self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
        "    \n",
        "  def init_hidden(self): \n",
        "      return torch.zeros(1, self.batch_size, self.hidden_size)\n",
        "    \n",
        "  def forward(self, input_data, hidden, fixed_effects, gaussian_likelihood): \n",
        "    z = input_data[:,0]\n",
        "    x = input_data[:,1:]\n",
        "    rnn_out, hidden = self.rnn(x.view(len(x), self.batch_size, -1))\n",
        "    sigma = self.linear(rnn_out).view(-1)\n",
        "    sigma = torch.abs(sigma)\n",
        "    r = torch.zeros(sigma.shape[0])\n",
        "    for i in range(sigma.shape[0]):\n",
        "      r[i] = torch.distributions.normal.Normal(0, sigma[i]).rsample()\n",
        "    u = fixed_effects + r\n",
        "    \n",
        "    if gaussian_likelihood == True: \n",
        "      log_lik = self.log_likelihood_Gaussian(z, fixed_effects, sigma)\n",
        "    \n",
        "    else: \n",
        "      log_lik = self.log_likelihood_nonGaussian()\n",
        "    \n",
        "    return log_lik\n",
        "   \n",
        "  def log_likelihood_Gaussian(self, z, f, sigma):\n",
        "    log_p = torch.zeros(len(z))\n",
        "    for i in range(len(z)): \n",
        "      pdf = torch.exp(torch.distributions.normal.Normal(0, sigma[i]).log_prob(z[i] - f[i]))\n",
        "      #scale the likelihood to 0-1\n",
        "      norm_constant = torch.exp(torch.distributions.normal.Normal(0, sigma[i]).log_prob(0))\n",
        "      log_p[i] = torch.log(pdf/norm_constant)\n",
        "      \n",
        "    #set neg infinite values to -10 to avoid batch loss becoming infinite \n",
        "    log_p[log_p == -float(\"inf\")] = -10\n",
        "    \n",
        "    log_lik = torch.sum(log_p)\n",
        "      \n",
        "    return log_lik\n",
        "  \n",
        "  \n",
        "  def log_likelihood_nonGaussian(self, ):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KpePhAU974O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_model = GlobalEffects(input_size = 64, num_factors = 10 , hidden_size = 1)  \n",
        "local_model = DF_RNN(64, hidden_size = 5, batch_size = 1, output_size = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGd_qELTtqx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 50\n",
        "num_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWZumPtm7ZCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a3fd1f3-9002-4be1-ef3f-21fcec0759d8"
      },
      "source": [
        "#loss_function = torch.nn.NLLLoss()\n",
        "optimiser = torch.optim.SGD(list(global_model.parameters()) + list(local_model.parameters()), lr = learning_rate)\n",
        "\n",
        "#optimiser_global = torch.optim.SGD(global_model.parameters(), lr = learning_rate)\n",
        "#optimiser_local = torch.optim.SGD(local_model.parameters(), lr = learning_rate)\n",
        "\n",
        "train_loss = []\n",
        "\n",
        "for t in range(num_epochs): \n",
        "  global_model.zero_grad()\n",
        "  global_hidden = global_model.init_hidden()\n",
        "  \n",
        "  local_model.zero_grad()\n",
        "  local_hidden = local_model.init_hidden()\n",
        "  \n",
        "  data_batch = covars_data\n",
        "  \n",
        "  neg_batch_loss = 0 \n",
        "  fixed_effects = global_model(data_batch[0], global_hidden)\n",
        "  \n",
        "  for i in range(batch_size): \n",
        "    data = data_batch[i]\n",
        "    \n",
        "    #fixed_effects = global_model(data, global_hidden) # this could be taken out of the for loop as if we keep the same covariate structure for  every series then g will be the same for each TS\n",
        "    log_lik = local_model(data, local_hidden, fixed_effects, gaussian_likelihood = True)\n",
        "    \n",
        "    neg_batch_loss += log_lik\n",
        "    \n",
        "  batch_loss = -1*neg_batch_loss\n",
        "    \n",
        "  optimiser.zero_grad()\n",
        "  \n",
        "  batch_loss.backward()\n",
        "\n",
        "  optimiser.step()\n",
        "\n",
        "  train_loss.append(batch_loss.item())\n",
        "  \n",
        "  print(\"Epoch: \", t, \"loss: \", batch_loss.item())"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 loss:  9.581878662109375\n",
            "Epoch:  1 loss:  83.43400573730469\n",
            "Epoch:  2 loss:  321.53192138671875\n",
            "Epoch:  3 loss:  0.5627274513244629\n",
            "Epoch:  4 loss:  0.5013756155967712\n",
            "Epoch:  5 loss:  0.45707476139068604\n",
            "Epoch:  6 loss:  0.42540493607521057\n",
            "Epoch:  7 loss:  0.40233591198921204\n",
            "Epoch:  8 loss:  0.3853866755962372\n",
            "Epoch:  9 loss:  0.3733386993408203\n",
            "Epoch:  10 loss:  0.3643936216831207\n",
            "Epoch:  11 loss:  0.3575715720653534\n",
            "Epoch:  12 loss:  0.35273754596710205\n",
            "Epoch:  13 loss:  0.34917423129081726\n",
            "Epoch:  14 loss:  0.3461487293243408\n",
            "Epoch:  15 loss:  0.34426867961883545\n",
            "Epoch:  16 loss:  0.34255486726760864\n",
            "Epoch:  17 loss:  0.34125643968582153\n",
            "Epoch:  18 loss:  0.3401396572589874\n",
            "Epoch:  19 loss:  0.33927083015441895\n",
            "Epoch:  20 loss:  0.3384381830692291\n",
            "Epoch:  21 loss:  0.33779093623161316\n",
            "Epoch:  22 loss:  0.3371810019016266\n",
            "Epoch:  23 loss:  0.3367464542388916\n",
            "Epoch:  24 loss:  0.3361092507839203\n",
            "Epoch:  25 loss:  0.33555468916893005\n",
            "Epoch:  26 loss:  0.33526474237442017\n",
            "Epoch:  27 loss:  0.3349865972995758\n",
            "Epoch:  28 loss:  0.33458492159843445\n",
            "Epoch:  29 loss:  0.3343857228755951\n",
            "Epoch:  30 loss:  0.33391255140304565\n",
            "Epoch:  31 loss:  0.33345285058021545\n",
            "Epoch:  32 loss:  0.3331950008869171\n",
            "Epoch:  33 loss:  0.3329640328884125\n",
            "Epoch:  34 loss:  0.33262819051742554\n",
            "Epoch:  35 loss:  0.33213576674461365\n",
            "Epoch:  36 loss:  0.3319494426250458\n",
            "Epoch:  37 loss:  0.33145302534103394\n",
            "Epoch:  38 loss:  0.33134934306144714\n",
            "Epoch:  39 loss:  0.33117595314979553\n",
            "Epoch:  40 loss:  0.3306181728839874\n",
            "Epoch:  41 loss:  0.33060893416404724\n",
            "Epoch:  42 loss:  0.330189973115921\n",
            "Epoch:  43 loss:  0.32994502782821655\n",
            "Epoch:  44 loss:  0.32961586117744446\n",
            "Epoch:  45 loss:  0.32931429147720337\n",
            "Epoch:  46 loss:  0.32906070351600647\n",
            "Epoch:  47 loss:  0.3288708031177521\n",
            "Epoch:  48 loss:  0.3287564516067505\n",
            "Epoch:  49 loss:  0.32836824655532837\n",
            "Epoch:  50 loss:  0.3283461332321167\n",
            "Epoch:  51 loss:  0.3278752863407135\n",
            "Epoch:  52 loss:  0.3276619613170624\n",
            "Epoch:  53 loss:  0.3272249400615692\n",
            "Epoch:  54 loss:  0.3270394504070282\n",
            "Epoch:  55 loss:  0.32706496119499207\n",
            "Epoch:  56 loss:  0.32672637701034546\n",
            "Epoch:  57 loss:  0.32641348242759705\n",
            "Epoch:  58 loss:  0.3260635435581207\n",
            "Epoch:  59 loss:  0.32610926032066345\n",
            "Epoch:  60 loss:  0.3256596028804779\n",
            "Epoch:  61 loss:  0.3255240321159363\n",
            "Epoch:  62 loss:  0.3251590430736542\n",
            "Epoch:  63 loss:  0.32516446709632874\n",
            "Epoch:  64 loss:  0.32483339309692383\n",
            "Epoch:  65 loss:  0.32456153631210327\n",
            "Epoch:  66 loss:  0.3243439793586731\n",
            "Epoch:  67 loss:  0.32414132356643677\n",
            "Epoch:  68 loss:  0.32387876510620117\n",
            "Epoch:  69 loss:  0.3237077593803406\n",
            "Epoch:  70 loss:  0.3235808312892914\n",
            "Epoch:  71 loss:  0.3231508135795593\n",
            "Epoch:  72 loss:  0.32306772470474243\n",
            "Epoch:  73 loss:  0.3228159546852112\n",
            "Epoch:  74 loss:  0.32267698645591736\n",
            "Epoch:  75 loss:  0.32261642813682556\n",
            "Epoch:  76 loss:  0.3222673237323761\n",
            "Epoch:  77 loss:  0.3220088481903076\n",
            "Epoch:  78 loss:  0.32176876068115234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-fda9b20bbf33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#log_lik = local_model(data, local_hidden, fixed_effects, gaussian_likelihood = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#print('log_lik', log_lik)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlog_lik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#neg_batch_loss += log_lik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-a5765c5faf1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data, hidden, fixed_effects, gaussian_likelihood)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             result = _impl(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 211\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             result = _impl(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53UOowhFy3K7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "fa51e776-6df4-4e92-f7bb-e9578072d066"
      },
      "source": [
        "plt.plot(train_loss)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1b4e89e588>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGGdJREFUeJzt3X+MXeV95/H35/6wAyYJBk9d13bW\ntHWL6G5jsrMsUdJVGtqG0KpOpTYLWrVWReVqRbTJKtIKWmnbSBupldqwrbSL5C5saJtC2PxYEKI/\nqINUdaVAB0KIwWHjJlDsNXhsfsUmmR/3fveP89zxnfGduXd++d55ns9LGs2555zxfO07/vjx9zzn\nOYoIzMwsX7VhF2BmZuvLQW9mljkHvZlZ5hz0ZmaZc9CbmWXOQW9mljkHvZlZ5hz0ZmaZc9CbmWWu\nMewCALZt2xZ79uwZdhlmZhvKk08+eToixvqdNxJBv2fPHiYmJoZdhpnZhiLpxUHOc+vGzCxzDnoz\ns8w56M3MMuegNzPLnIPezCxzDnozs8w56M3MMpd10P+/17/HV775yrDLMDMbqqyD/s+/+iL//s+f\nGnYZZmZDlXXQvzXdYmq2jR+AbmYlyzroZ1ptAFptB72ZlSvroJ+erYJ+puWgN7Ny5R30aUQ/024P\nuRIzs+HJO+jTiH7WI3ozK1ghQe8RvZmVK++gn2vdeERvZuXKO+g9ojczyzzoW551Y2bWN+glvU3S\nE5K+LulZSZ9K+6+S9LikY5I+L2lT2r85vT6Wju9Z39/C4uZG9J51Y2YFG2REPwV8MCLeDewDbpR0\nPfD7wJ0R8aPAa8Ct6fxbgdfS/jvTeUPhWTdmZgMEfVTOppfN9BHAB4EvpP33Ah9J2/vTa9LxGyRp\nzSpehvOtG4/ozaxcA/XoJdUlPQ2cAh4F/hF4PSJm0ynHgZ1peyfwEkA6/gZwZY9f86CkCUkTk5OT\nq/tdLGJmrnXjEb2ZlWugoI+IVkTsA3YB1wFXr/YbR8ShiBiPiPGxsbHV/nI9eURvZrbMWTcR8Trw\nGPBe4HJJjXRoF3AibZ8AdgOk4+8EzqxJtcs05R69mdlAs27GJF2eti8BfhY4ShX4v5xOOwA8mLYf\nSq9Jx78SQ1on2LNuzMyg0f8UdgD3SqpT/cPwQEQ8LOk54H5J/wX4GnB3Ov9u4M8kHQNeBW5eh7r7\nigjPozczY4Cgj4hngGt77P82Vb9+4f7vA7+yJtWtwmw76Pw/wq0bMytZtnfGdl+AdevGzEqWbdB3\n+vPg1o2Zla2QoPeI3szKlW3QT3UFvVevNLOSZRv00y23bszMIOOg98VYM7NKtkHvi7FmZpUigt7z\n6M2sZGUEvVs3ZlawbIN+yhdjzcyAjIN+2tMrzcyAjIN+/qwbj+jNrFzZBr3vjDUzqxQR9J51Y2Yl\nyzfo0yj+kmadGc+6MbOC5Rv0aUS/ZXPdI3ozK1q+QZ9G9JduangevZkVLd+gn+0Efd3z6M2saFkH\nfbMumvWa59GbWdEyD/oajbo8j97MipZv0LfabGrUaNZq86ZampmVJt+gn22zySN6M7P+QS9pt6TH\nJD0n6VlJH0/7f1fSCUlPp4+bur7mDknHJD0v6UPr+RtYTGdE33CP3swK1xjgnFngkxHxlKS3A09K\nejQduzMi/qD7ZEnXADcDPwH8EPC3kn4sIlprWXg/07Od1o0868bMitZ3RB8RJyPiqbT9XeAosHOJ\nL9kP3B8RUxHxHeAYcN1aFLsc81s3HtGbWbmW1aOXtAe4Fng87fqYpGck3SNpa9q3E3ip68uO0+Mf\nBkkHJU1ImpicnFx24f3Mb914RG9m5Ro46CVdBnwR+EREvAncBfwIsA84Cfzhcr5xRByKiPGIGB8b\nG1vOlw6kM6Jv1uS1bsysaAMFvaQmVch/LiK+BBARr0REKyLawJ9wvj1zAtjd9eW70r6LasYjejMz\nYLBZNwLuBo5GxGe69u/oOu2XgCNp+yHgZkmbJV0F7AWeWLuSBzN3Mbbui7FmVrZBZt28D/hV4BuS\nnk77fgu4RdI+IIAXgN8EiIhnJT0APEc1Y+e2iz3jBmAq3RnbrNd8MdbMitY36CPi7wH1OPTIEl/z\naeDTq6hr1eYuxtbcujGzsmV9Z+zmeqd14xG9mZUr66CvLsZ6CQQzK1u2QT/T1bpptYMIh72ZlSnb\noJ+bR1+vLi945o2ZlSrfoG+1aaZ59IBn3phZsbIM+nY7mGlFtdZNzSN6MytblkHfeTB4dcNUGtF7\n5o2ZFSrLoO9Mp9ycZt1U+zyiN7MyZRn0nUcHdh4lCHguvZkVK8+gT6HeeTg44Ln0ZlasPIO+M6Kv\nd8268YjezAo1yKJmG05368azbsysdFkG/VRX0NfUad14RG9mZcoy6Ge6plee3+cRvZmVKcug7+7R\nd7hHb2alyjPou0b07TTbxrNuzKxUeQZ914h+VlXAex69mZUq76Bv1FAKeD9lysxKlWfQd7VuOsvQ\ne9aNmZUqz6Dvat10evSedWNmpcoz6LtG9LNzF2M9ojezMvVdAkHSbkmPSXpO0rOSPp72XyHpUUnf\nSp+3pv2S9MeSjkl6RtJ71vs3sdC8JRB8Z6yZFW6QtW5mgU9GxDXA9cBtkq4BbgcOR8Re4HB6DfBh\nYG/6OAjcteZV9zFv9cq5tW4c9GZWpr5BHxEnI+KptP1d4CiwE9gP3JtOuxf4SNreD/xpVL4KXC5p\nx5pXvoR5a93UvQSCmZVtWatXStoDXAs8DmyPiJPp0MvA9rS9E3ip68uOp30XzUyrjQSNmrrWo/eI\n3szKNHDQS7oM+CLwiYh4s/tYRASwrCSVdFDShKSJycnJ5XxpX1OtNs16DUldT5jyiN7MyjRQ0Etq\nUoX85yLiS2n3K52WTPp8Ku0/Aezu+vJdad88EXEoIsYjYnxsbGyl9fc0Pdtmc+rNz7VuHPRmVqhB\nZt0IuBs4GhGf6Tr0EHAgbR8AHuza/2tp9s31wBtdLZ6LYnq2PbdypVs3Zla6QebRvw/4VeAbkp5O\n+34L+D3gAUm3Ai8CH03HHgFuAo4BbwG/vqYVD6A76Gs1UZMvxppZufoGfUT8PaBFDt/Q4/wAbltl\nXasy02rPW4u+Ua95eqWZFSvPZ8ami7EdzZrcujGzYuUZ9LPteQ8daTZqbt2YWbGyDPqp2QWtm1rN\nI3ozK1aWQT+9IOibdXl6pZkVK8+gb7XZPO9irPwoQTMrVpZBP3PBxdia74w1s2JlGfQLL8Y26vL0\nSjMrVr5Bv+BirGfdmFmpigj6Zt3z6M2sXHkGfa87Yz2iN7NCZRn0Uwt79L4z1swKlmXQL1zrplmv\neR69mRUry6DvOevG8+jNrFDZBf1sq007uGDWTec5smZmpcku6Kdb5x8M3tH0iN7MCpZf0KeR+/zW\njXv0Zlau/II+BXqz4fXozcwgx6BPI/rN9QWzbjyP3swKlW3Qb1q4eqVH9GZWqPyCvufFWK9eaWbl\nyi/oe12MrXnWjZmVK9ugby5c68atGzMrVN+gl3SPpFOSjnTt+11JJyQ9nT5u6jp2h6Rjkp6X9KH1\nKnwxc62b+oLVK30x1swKNciI/rPAjT323xkR+9LHIwCSrgFuBn4ifc1/l1Rfq2IH0fNibK1GBLTc\nvjGzAvUN+oj4O+DVAX+9/cD9ETEVEd8BjgHXraK+ZZubXrlg1g3gC7JmVqTV9Og/JumZ1NrZmvbt\nBF7qOud42nfRLLYEAuALsmZWpJUG/V3AjwD7gJPAHy73F5B0UNKEpInJyckVlnGh3rNuqm0vg2Bm\nJVpR0EfEKxHRiog28Cecb8+cAHZ3nbor7ev1axyKiPGIGB8bG1tJGT3N9FoCYa514xG9mZVnRUEv\naUfXy18COjNyHgJulrRZ0lXAXuCJ1ZW4PIstagZ4GQQzK1Kj3wmS7gM+AGyTdBz4HeADkvYBAbwA\n/CZARDwr6QHgOWAWuC0iWutTem9TPWfdpB69R/RmVqC+QR8Rt/TYffcS538a+PRqilqNzsXYzQuW\nQOg+ZmZWknzvjK1fOL3SI3ozK1GWQV+viXpq18D5WTeeR29mJcou6Gda8x8MDrCp4Xn0Zlau7IJ+\nerY970IseB69mZUtv6Bv9Qh6z6M3s4JlF/RTsxe2bpqeR29mBcsu6Hu3bjzrxszKlV3Q97oY2xnR\ne9aNmZUou6DvOaL36pVmVrD8gr7XxVjPozezguUX9D0vxrpHb2blyjLomxe0bjzrxszKlV3Q95xe\nWfM8ejMrV3ZBP9Nqz1u5ErpG9O7Rm1mBsgv6pe6M9awbMytRfkHfs3XTmXXjoDez8uQZ9IuN6N26\nMbMCZRn0zXrvJRA8j97MSpRd0M+04oIRvSQaNTHjHr2ZFSiroI+InhdjoVrvxq0bMytRVkHf68Hg\nHY26fDHWzIqUV9CnB4MvnHUDaUTvO2PNrEB9g17SPZJOSTrSte8KSY9K+lb6vDXtl6Q/lnRM0jOS\n3rOexS/UCfrO2jbdGjV5rRszK9IgI/rPAjcu2Hc7cDgi9gKH02uADwN708dB4K61KXMwndbNpkb9\ngmPNes2tGzMrUt+gj4i/A15dsHs/cG/avhf4SNf+P43KV4HLJe1Yq2L7mZmtgrzXxdhGXW7dmFmR\nVtqj3x4RJ9P2y8D2tL0TeKnrvONp30Ux3WoBiwS9WzdmVqhVX4yNiACWnaCSDkqakDQxOTm52jKA\nauVKWPxirG+YMrMSrTToX+m0ZNLnU2n/CWB313m70r4LRMShiBiPiPGxsbEVljHf3KybRo+LsXV5\nUTMzK9JKg/4h4EDaPgA82LX/19Lsm+uBN7paPOvu/PTKCy/GNmoe0ZtZmRr9TpB0H/ABYJuk48Dv\nAL8HPCDpVuBF4KPp9EeAm4BjwFvAr69DzYvqzKrpfWese/RmVqa+QR8Rtyxy6IYe5wZw22qLWqml\nL8b6hikzK1Mxd8Z6CQQzK1VWQT8362axRc08ojezAmUV9EuO6D2P3swKlVfQt5Ye0XvWjZmVKK+g\nX7J14x69mZUpq6B//a0ZAN7xtgsnEzX84BEzK1RWQf/quWm2Xtqk0XMJBD9K0MzKlFXQnzk3xZWX\nbe55rFHziN7MypRV0J8+O82VWzb1PNbwnbFmVqisgv7M2Sm2LTKib9ZrzHgevZkVKK+gPzfNlZct\nMqL3PHozK1Q2QT/TavP6WzNcuWWRHn29xmw7qJbjMTMrRzZB/9q5aQCuWGRE36xVa9R7TXozK002\nQX/6bBX02xa9GFv9Vt2+MbPSZBP0Z85NASw6vbJZr0b0viBrZqXJJ+jTiH6pi7HgEb2ZlSeboD99\nthrRb1viYizgm6bMrDjZBP2Zc9M0auIdl/R+aNb51o1H9GZWlnyC/uwUV162CUk9jzdqHtGbWZky\nCvrpRefQQ7UEAuClis2sONkE/ekl7oqFagkEwI8TNLPiZBP0S61zA11B7xG9mRWm95XLAUl6Afgu\n0AJmI2Jc0hXA54E9wAvARyPitdWV2d+ZJVauhPOtm2n36M2sMGsxov/piNgXEePp9e3A4YjYCxxO\nr9fVW9OzfG+mtejNUgDNmkf0Zlam9Wjd7AfuTdv3Ah9Zh+8xT7+bpeD8iN6zbsysNKsN+gD+RtKT\nkg6mfdsj4mTafhnY3usLJR2UNCFpYnJyclVFzN0steTFWM+jN7MyrapHD7w/Ik5I+gHgUUnf7D4Y\nESGpZ7JGxCHgEMD4+Piq0nduRL/U9ErPozezQq1qRB8RJ9LnU8CXgeuAVyTtAEifT622yH7OL2jW\nv3XjefRmVpoVB72kLZLe3tkGfg44AjwEHEinHQAeXG2R/ZweYETvefRmVqrVtG62A19OSw40gL+I\niL+S9A/AA5JuBV4EPrr6Mpd25uw0WzbVuWRTfdFzvHqlmZVqxUEfEd8G3t1j/xnghtUUtVxnzk0t\nObUSzo/oZ9yjN7PCZHFn7JmzSy9/AF3TKz3rxswKk0XQnz47tWR/HjzrxszKlUXQnzk3veQceuia\nR+8evZkVZsMHfbsdvNpn5UroesKUZ92YWWE2fNC/8b0ZWu0YoHXjEb2ZlWnDB/0gN0uBlyk2s3Jt\n+KDv3Cy11Fr0APWaqMmtGzMrz4YP+kFWruxo1Gtu3ZhZcTZ+0HdaN3169ADNmjy90syKs+GD/vTZ\naSTYemmz77nViN5Bb2Zl2fBBf+bsFFsv3TQ3fXIpzbq8Hr2ZFSeDoF/6WbHdGrWaWzdmVpyNH/Tn\npga6EAvVejeeXmlmpdn4QX92uu/KlR3Nes2tGzMrzoYP+tNnp9g2cOvGs27MrDwbOuinZ9u8+f3Z\ngUf0nkdvZiXa0EH/6rnBb5aCataN74w1s9Js6KA/fXbwm6Wg07rxiN7MyrKhg/7Muc46N4POuvEN\nU2ZWng0d9Ge/P0uzrmXMupEfJWhmxVnxw8FHwc//5A5u+hc/OPD51Q1Ts+tYkZnZ6Fm3Eb2kGyU9\nL+mYpNvX8fsgaaBzm3V51o2ZFWddgl5SHfhvwIeBa4BbJF2zHt9rORq1mmfdmFlx1mtEfx1wLCK+\nHRHTwP3A/nX6XgNrNmqedWNmxVmvHv1O4KWu18eBf71O32tgmxs1vn36HD/223+J1HnqlJCgpuoJ\nVJIQoIXbKH2udNpFC7tGnddikeMsPH/pttNgTamVnHxxjXBpG9qgbUsbnn7v0L/9V7v5jZ/64XWt\nYWgXYyUdBA4CvOtd77oo3/M3fuoqfuDtm2kHRATtCFptCIIIaLVjbjuozokgva62oXOs2jdPzPtE\nRPQ6fP51n/9cLOf/Hgu/1ygZ3co2OP/BjrwLMqKHfo9BXQvrFfQngN1dr3elfXMi4hBwCGB8fPyi\n/Mhe/YPv4Oob33ExvpWZ2chYrx79PwB7JV0laRNwM/DQOn0vMzNbwrqM6CNiVtLHgL8G6sA9EfHs\nenwvMzNb2rr16CPiEeCR9fr1zcxsMBt6CQQzM+vPQW9mljkHvZlZ5hz0ZmaZc9CbmWVOo3BHpaRJ\n4MUVfvk24PQalrOWXNvKjHJtMNr1ubaV2ai1/bOIGOv3C4xE0K+GpImIGB92Hb24tpUZ5dpgtOtz\nbSuTe21u3ZiZZc5Bb2aWuRyC/tCwC1iCa1uZUa4NRrs+17YyWde24Xv0Zma2tBxG9GZmtoQNHfQX\n6wHkA9Zyj6RTko507btC0qOSvpU+bx1SbbslPSbpOUnPSvr4qNQn6W2SnpD09VTbp9L+qyQ9nt7b\nz6flrodCUl3S1yQ9PEq1SXpB0jckPS1pIu0b+nua6rhc0hckfVPSUUnvHYXaJP14+vPqfLwp6ROj\nUFuq7z+mvwdHJN2X/n6s+udtwwb9CD6A/LPAjQv23Q4cjoi9wOH0ehhmgU9GxDXA9cBt6c9qFOqb\nAj4YEe8G9gE3Sroe+H3gzoj4UeA14NYh1NbxceBo1+tRqu2nI2Jf1/S7UXhPAf4I+KuIuBp4N9Wf\n39Bri4jn05/XPuBfAm8BXx6F2iTtBP4DMB4R/5xqifebWYuft+pxeRvvA3gv8Nddr+8A7hhyTXuA\nI12vnwd2pO0dwPPD/nNLtTwI/Oyo1QdcCjxF9Xzh00Cj13t9kWvaRfUX/4PAw1SPAB2V2l4Ati3Y\nN/T3FHgn8B3SNcBRqm1BPT8H/J9RqY3zz9q+gmoJ+YeBD63Fz9uGHdHT+wHkO4dUy2K2R8TJtP0y\nsH2YxQBI2gNcCzzOiNSXWiNPA6eAR4F/BF6PiNl0yjDf2/8K/CegnV5fyejUFsDfSHoyPYMZRuM9\nvQqYBP5nann9D0lbRqS2bjcD96XtodcWESeAPwD+CTgJvAE8yRr8vG3koN9QovrneKhTnCRdBnwR\n+EREvNl9bJj1RUQrqv9K7wKuA64eRh0LSfoF4FREPDnsWhbx/oh4D1X78jZJ/6b74BDf0wbwHuCu\niLgWOMeCVsiw/z6kPvcvAv9r4bFh1ZauC+yn+ofyh4AtXNgOXpGNHPR9H0A+Al6RtAMgfT41rEIk\nNalC/nMR8aVRqw8gIl4HHqP67+nlkjpPQBvWe/s+4BclvQDcT9W++aMRqa0zAiQiTlH1ma9jNN7T\n48DxiHg8vf4CVfCPQm0dHwaeiohX0utRqO1ngO9ExGREzABfovoZXPXP20YO+o3wAPKHgANp+wBV\nb/yikyTgbuBoRHym69DQ65M0JunytH0J1bWDo1SB/8vDrC0i7oiIXRGxh+rn6ysR8e9GoTZJWyS9\nvbNN1W8+wgi8pxHxMvCSpB9Pu24AnhuF2rrcwvm2DYxGbf8EXC/p0vR3tvPntvqft2FeDFmDixc3\nAf+Xqqf720Ou5T6qvtoM1YjmVqp+7mHgW8DfAlcMqbb3U/1X9Bng6fRx0yjUB/wk8LVU2xHgP6f9\nPww8ARyj+u/15iG/vx8AHh6V2lINX08fz3Z+/kfhPU117AMm0vv6v4GtI1TbFuAM8M6ufaNS26eA\nb6a/C38GbF6LnzffGWtmlrmN3LoxM7MBOOjNzDLnoDczy5yD3swscw56M7PMOejNzDLnoDczy5yD\n3swsc/8fnY/dbKv904gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}