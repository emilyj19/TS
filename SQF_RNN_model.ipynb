{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQF_RNN_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyj19/TS/blob/master/SQF_RNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPtSnNPJ7eTH",
        "colab_type": "code",
        "outputId": "760cf7cf-2b0b-461d-f067-1d02fa407331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScIt9ImB8cav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvoZxQX7-G1",
        "colab_type": "code",
        "outputId": "4e0894a4-25cd-40c4-e4a0-51b52bca22fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTbcnwJm7_lD",
        "colab_type": "code",
        "outputId": "12b0eb1b-37aa-4b73-a635-928e72ce9b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "root_path = 'gdrive/My Drive/FinancialTS/JPmarket_dataset.npz' \n",
        "data = np.load(root_path)\n",
        "data.files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_ratios', 'test_ratios', 'train_volumes', 'test_volumes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8SDMHW78HXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ratios = data['train_ratios']\n",
        "test_ratios = data['test_ratios']\n",
        "train_vols = data['train_volumes']\n",
        "test_vols = data['test_volumes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI540y0JQBpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mini_train_ratios = train_ratios[0:50]\n",
        "mini_test_ratios = test_ratios[0:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItoKOzL8K-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#need to change this to create windows for all stocks \n",
        "\n",
        "def split_data(n_days, data): #data for one stock in the form [days, bins]\n",
        "  no_ts = data.shape[0]-n_days\n",
        "  length_ts = n_days*data.shape[1]\n",
        "  new_data = np.zeros((no_ts,length_ts))\n",
        "  for j in range(no_ts):\n",
        "    for i in range(n_days): \n",
        "      new_data[j,64*i:64*i + 64] = data[j+i,:]\n",
        "  \n",
        "  return new_data\n",
        "\n",
        "training_data = split_data(180, train_ratios[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_oSS9u8N8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_test_data(train_data, test_data, n_days, stock_index):\n",
        "  all_data = np.concatenate((train_data, test_data), axis = 1)\n",
        "  \n",
        "  one_data = all_data[stock_index]\n",
        "  \n",
        "  #now want to take only sets of n_days which contain the test data too \n",
        "  no_ts = test_data.shape[1]\n",
        "  length_ts = n_days*one_data.shape[1]\n",
        "  new_data = np.zeros((no_ts, length_ts))\n",
        "  \n",
        "  for j in range(no_ts): \n",
        "    for i in range(n_days): \n",
        "      new_data[j, 64*i:64*i + 64] = one_data[train_data.shape[1]+j-180+i+1,:]\n",
        "      \n",
        "  return new_data\n",
        "\n",
        "\n",
        "testing_ratios = make_test_data(train_ratios, test_ratios, 180, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtpFnvDpUf-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.module): \n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, batch_size, output_size, num_layers):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
        "    \n",
        "  def init_hidden(self): \n",
        "    return (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
        "              torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
        "  \n",
        "  def forward(self, data):\n",
        "    lstm_out, self.hidden = self.lstm(data.view(len(data), self.batch_size, -1))\n",
        "    \n",
        "    return self.hidden #want the full sequence of hidden states as the output of the encoder layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyo_wg4EXhQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.module): \n",
        "  \n",
        "  # want the decoder to share the same weights as the encoder - we won't train this?\n",
        "  # but we need the decoder to take the output at the previous step to feed in as the next input \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bFMUAIKYrl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CRPS_loss(nn.module): \n",
        "  \n",
        "  def __init__(self, ): \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gZYO9Nb8On1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SQF_RNN(nn.module): \n",
        "  \n",
        "  def __init__(self, encoder, decoder, loss_fn): \n",
        "    super(SQF_RNN, self).__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder \n",
        "    self.loss_fn = loss_fn\n",
        "   \n",
        "    \n",
        "  \n",
        "  def forward(self, data): \n",
        "    output = self.encoder() #does this need to go within the class really?\n",
        "  \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUf_zUXgPBl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = \n",
        "learning_rate_decay = "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}