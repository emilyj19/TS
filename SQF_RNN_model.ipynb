{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQF_RNN_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyj19/TS/blob/master/SQF_RNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPtSnNPJ7eTH",
        "colab_type": "code",
        "outputId": "f0d77903-9055-4529-fe41-6a648cdb0e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rl6TAk4HBLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1092
        },
        "outputId": "88d76171-0c1d-4d6a-c9d0-76cc1b223d39"
      },
      "source": [
        "pip install gluonts"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonts\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/4a/f736941b23d639270121f0db30e2f1eed8d6986be5dd7166d396eecfd6ea/gluonts-0.1.4-py3-none-any.whl (222kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 4.8MB/s \n",
            "\u001b[?25hCollecting mxnet>=1.3.1 (from gluonts)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f4/bc147a1ba7175f9890523ff8f1a928a43ac8a79d5897a067158cac4d092f/mxnet-1.4.1-py2.py3-none-manylinux1_x86_64.whl (28.4MB)\n",
            "\u001b[K     |████████████████████████████████| 28.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.6/dist-packages (from gluonts) (4.28.1)\n",
            "Collecting pydantic==0.28.* (from gluonts)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/bc/fe7d98f0b4b1e72d0c444f343a798461c1f9d8656fb1c335416dbb8b7976/pydantic-0.28-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 22.3MB/s \n",
            "\u001b[?25hCollecting ujson>=1.35 (from gluonts)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 42.4MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.* (from gluonts)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.* in /usr/local/lib/python3.6/dist-packages (from gluonts) (3.0.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from gluonts) (0.24.2)\n",
            "Requirement already satisfied: holidays==0.9.* in /usr/local/lib/python3.6/dist-packages (from gluonts) (0.9.10)\n",
            "Requirement already satisfied: boto3==1.* in /usr/local/lib/python3.6/dist-packages (from gluonts) (1.9.167)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet>=1.3.1->gluonts) (2.21.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet>=1.3.1->gluonts)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==0.28.*->gluonts) (0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.*->gluonts) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.*->gluonts) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.*->gluonts) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.*->gluonts) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->gluonts) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from holidays==0.9.*->gluonts) (1.12.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3==1.*->gluonts) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3==1.*->gluonts) (1.12.167)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3==1.*->gluonts) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet>=1.3.1->gluonts) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet>=1.3.1->gluonts) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet>=1.3.1->gluonts) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet>=1.3.1->gluonts) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.*->gluonts) (41.0.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3==1.*->gluonts) (0.14)\n",
            "Building wheels for collected packages: ujson\n",
            "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "Successfully built ujson\n",
            "\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.53.post2 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: graphviz, numpy, mxnet, pydantic, ujson, gluonts\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed gluonts-0.1.4 graphviz-0.8.4 mxnet-1.4.1 numpy-1.14.6 pydantic-0.28 ujson-1.35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScIt9ImB8cav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvoZxQX7-G1",
        "colab_type": "code",
        "outputId": "25d678fb-3c27-473a-8517-57aa11bf4b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTbcnwJm7_lD",
        "colab_type": "code",
        "outputId": "2ada517d-0833-4464-994a-8fa603885985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "root_path = 'gdrive/My Drive/FinancialTS/JPmarket_dataset.npz' \n",
        "data = np.load(root_path)\n",
        "data.files"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_ratios', 'test_ratios', 'train_volumes', 'test_volumes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8SDMHW78HXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ratios = data['train_ratios']\n",
        "test_ratios = data['test_ratios']\n",
        "train_vols = data['train_volumes']\n",
        "test_vols = data['test_volumes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI540y0JQBpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mini_train_ratios = train_ratios[0:50]\n",
        "mini_test_ratios = test_ratios[0:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItoKOzL8K-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#need to change this to create windows for all stocks \n",
        "\n",
        "def split_data(n_days, data): #data for one stock in the form [days, bins]\n",
        "  no_ts = data.shape[0]-n_days\n",
        "  length_ts = n_days*data.shape[1]\n",
        "  new_data = np.zeros((no_ts,length_ts))\n",
        "  for j in range(no_ts):\n",
        "    for i in range(n_days): \n",
        "      new_data[j,64*i:64*i + 64] = data[j+i,:]\n",
        "  \n",
        "  return new_data\n",
        "\n",
        "training_data = split_data(180, train_ratios[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_oSS9u8N8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_test_data(train_data, test_data, n_days, stock_index):\n",
        "  all_data = np.concatenate((train_data, test_data), axis = 1)\n",
        "  \n",
        "  one_data = all_data[stock_index]\n",
        "  \n",
        "  #now want to take only sets of n_days which contain the test data too \n",
        "  no_ts = test_data.shape[1]\n",
        "  length_ts = n_days*one_data.shape[1]\n",
        "  new_data = np.zeros((no_ts, length_ts))\n",
        "  \n",
        "  for j in range(no_ts): \n",
        "    for i in range(n_days): \n",
        "      new_data[j, 64*i:64*i + 64] = one_data[train_data.shape[1]+j-180+i+1,:]\n",
        "      \n",
        "  return new_data\n",
        "\n",
        "\n",
        "testing_ratios = make_test_data(train_ratios, test_ratios, 180, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvkSNm358o4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getbd_from_theta(theta): \n",
        "  gamma, beta, delta = theta\n",
        "  \n",
        "  L = len(beta)\n",
        "  \n",
        "  b = torch.zeros(L)\n",
        "  for l in range(L): \n",
        "    if l == 0 : \n",
        "      b[l] = beta[l]\n",
        "    else: \n",
        "      b[l] = beta[l]-beta[l-1]\n",
        "  d = torch.zeros(L)\n",
        "  for l in range(1:L): \n",
        "    d[l] = torch.sum(delta[:l])\n",
        "    \n",
        "  return b, d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Q5oJFh2igo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crps_loss(theta, z):\n",
        "  gamma, beta, delta = theta\n",
        "  \n",
        "  L = len(beta)\n",
        "  \n",
        "  b, d = getbd_from_theta(theta)\n",
        "    \n",
        "  for l in range(L, 0, -1): \n",
        "    val = sqf(theta, d[l])\n",
        "    if val < z:\n",
        "      lo = l\n",
        "      break \n",
        "  \n",
        "  a_tilde = (z-gamma + torch.sum(b[:lo]*d[:lo]))/torch.sum(b[:lo])\n",
        "  \n",
        "  max_ = torch.max(torch.zeros(L)+a_tilde, d)\n",
        "  \n",
        "  bracket = (1/3)*(1-torch.pow(d, 3)) - d - torch.pow(max_,2) + 2*max_*d\n",
        "  \n",
        "  loss = (2*a_tilde - 1)*z + (1-2*a_tilde)*gamma + torch.sum(b*bracket)\n",
        "  \n",
        "  return loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7byuacN2kW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sqf(theta, quantile): \n",
        "  \n",
        "  #would expect beta and delta to be vectors length hidden_units\n",
        "  \n",
        "  gamma, beta, delta = theta\n",
        "  L = len(beta)\n",
        "  \n",
        "  b,d = getbd_from_theta(theta)\n",
        "  \n",
        "  max_ = torch.max(q-d, torch.zeros(L))\n",
        "  \n",
        "  qf = gamma + torch.sum(b*max_)\n",
        "  \n",
        "  return qf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtpFnvDpUf-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.module): \n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, batch_size, output_size, num_layers):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "    self.softmax = nn.functional.softmax\n",
        "    self.softplus = nn.functional.softplus\n",
        "    \n",
        "  def init_hidden(self): \n",
        "    return (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
        "              torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
        "  \n",
        "  def forward(self, data, hidden):\n",
        "    lstm_out, hidden = self.lstm(data.view(len(data), self.batch_size, -1))\n",
        "    fc_layer = self.Linear()\n",
        "    delta = self.softmax(fc_layer)\n",
        "    beta = self.softmax(fc_layer)\n",
        "    \n",
        "    theta = (delta, beta)\n",
        "    \n",
        "    \n",
        "    return theta, hidden #want the full sequence of hidden states as the output of the encoder layer NO we only want the last one?!\n",
        "  \n",
        "  ##does output of hidden give the outputs from every layer?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyo_wg4EXhQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.module): \n",
        "  \n",
        "  # want the decoder to share the same weights as the encoder - we won't train this?\n",
        "  # but we need the decoder to take the output at the previous step to feed in as the next input \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIa10RoikYuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TRAINING the encoder###\n",
        "encoder = Encoder(...)\n",
        "loss_function = crps_loss()\n",
        "encoder_optimiser = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "T = #decide this\n",
        "\n",
        "for i in range(num_epochs): \n",
        "  encoder.zero_grad()\n",
        "  encoder.hidden = encoder.init_hidden()\n",
        "  loss_t = 0 \n",
        "  \n",
        "  for t in range(T): \n",
        "    theta, encoder_hidden = encoder(input_data[t], encoder_hidden)\n",
        "    loss_t += loss_function(theta, input_data[t+1])\n",
        "    \n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "    \n",
        "   \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUf_zUXgPBl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = \n",
        "learning_rate_decay = "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}