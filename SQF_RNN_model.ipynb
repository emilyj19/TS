{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQF_RNN_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyj19/TS/blob/master/SQF_RNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPtSnNPJ7eTH",
        "colab_type": "code",
        "outputId": "111c67a3-2eb5-402b-94c1-dd6e98982b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScIt9ImB8cav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZvoZxQX7-G1",
        "colab_type": "code",
        "outputId": "61c2f210-ca9f-4df1-f31c-85e5978de68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTbcnwJm7_lD",
        "colab_type": "code",
        "outputId": "dbb17116-4d05-4680-d44a-1938e9d8a7d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "root_path = 'gdrive/My Drive/FinancialTS/JPmarket_dataset.npz' \n",
        "data = np.load(root_path)\n",
        "data.files"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_ratios', 'test_ratios', 'train_volumes', 'test_volumes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8SDMHW78HXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ratios = data['train_ratios']\n",
        "test_ratios = data['test_ratios']\n",
        "train_vols = data['train_volumes']\n",
        "test_vols = data['test_volumes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI540y0JQBpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mini_train_ratios = train_ratios[0:50]\n",
        "mini_test_ratios = test_ratios[0:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItoKOzL8K-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#need to change this to create windows for all stocks \n",
        "\n",
        "def split_data(n_days, data): #data for one stock in the form [days, bins]\n",
        "  no_ts = data.shape[0]-n_days\n",
        "  length_ts = n_days*data.shape[1]\n",
        "  new_data = np.zeros((no_ts,length_ts))\n",
        "  for j in range(no_ts):\n",
        "    for i in range(n_days): \n",
        "      new_data[j,64*i:64*i + 64] = data[j+i,:]\n",
        "  \n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_oSS9u8N8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_test_data(train_data, test_data, n_days, stock_index):\n",
        "  all_data = np.concatenate((train_data, test_data), axis = 1)\n",
        "  \n",
        "  one_data = all_data[stock_index]\n",
        "  \n",
        "  #now want to take only sets of n_days which contain the test data too \n",
        "  no_ts = test_data.shape[1]\n",
        "  length_ts = n_days*one_data.shape[1]\n",
        "  new_data = np.zeros((no_ts, length_ts))\n",
        "  \n",
        "  for j in range(no_ts): \n",
        "    for i in range(n_days): \n",
        "      new_data[j, 64*i:64*i + 64] = one_data[train_data.shape[1]+j-180+i+1,:]\n",
        "      \n",
        "  return new_data\n",
        "\n",
        "\n",
        "testing_ratios = make_test_data(train_ratios, test_ratios, 180, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvkSNm358o4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getbd_from_theta(theta): \n",
        "  gamma, beta, delta = theta\n",
        "  \n",
        "  L = len(beta)\n",
        "  \n",
        "  b = torch.zeros(L)\n",
        "  for l in range(L): \n",
        "    if l == 0 : \n",
        "      b[l] = beta[l]\n",
        "    else: \n",
        "      b[l] = beta[l]-beta[l-1]\n",
        "  d = torch.zeros(L)\n",
        "  for l in range(1,L): \n",
        "    d[l] = torch.sum(delta[:l])\n",
        "    \n",
        "  return b, d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Q5oJFh2igo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crps_loss(theta, z):\n",
        "  gamma, beta, delta = theta\n",
        "  \n",
        "  L = len(beta)\n",
        "  \n",
        "  b, d = getbd_from_theta(theta)\n",
        "  \n",
        "  lo = 0 \n",
        "    \n",
        "  for l in range(L-1, -1, -1): \n",
        "    val = sqf(theta, d[l])\n",
        "    if val < z:\n",
        "      lo = l\n",
        "      break \n",
        "  \n",
        "  a_tilde = (z-gamma + torch.sum(b[:lo]*d[:lo]))/torch.sum(b[:lo+1])\n",
        "  \n",
        "  max_ = torch.max(torch.zeros(L)+a_tilde, d)\n",
        "  #print('max', max_)\n",
        "  \n",
        "  bracket = (1/3)*(1-torch.pow(d, 3)) - d - torch.pow(max_,2) + 2*max_*d\n",
        "  #print('bracket', bracket)\n",
        "  \n",
        "  loss = (2*a_tilde - 1)*z + (1-2*a_tilde)*gamma + torch.sum(b*bracket)\n",
        "  #print(loss.item())\n",
        "  \n",
        "  return loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7byuacN2kW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sqf(theta, quantile): \n",
        "  \n",
        "  #would expect beta and delta to be vectors length hidden_units\n",
        "  \n",
        "  gamma, beta, delta = theta\n",
        "  L = len(beta)\n",
        "  \n",
        "  b,d = getbd_from_theta(theta)\n",
        "    \n",
        "  max_ = torch.max(quantile-d, torch.zeros(L))\n",
        "  \n",
        "  qf = gamma + torch.sum(b*max_)\n",
        "  \n",
        "  return qf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtpFnvDpUf-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module): \n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, batch_size, output_size, num_layers):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
        "    self.linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "    self.dense = nn.Linear(self.hidden_size, 1)\n",
        "    self.softmax = nn.functional.softmax\n",
        "    self.softplus = nn.functional.softplus\n",
        "    \n",
        "  def init_hidden(self): \n",
        "    return torch.zeros(self.num_layers, 1, self.hidden_size)\n",
        "  \n",
        "  def forward(self, data, hidden):\n",
        "    lstm_out, hidden = self.lstm(data.view(1, 1, -1))\n",
        "    fc_layer = self.linear(lstm_out.view(-1)) \n",
        "    delta = self.softmax(fc_layer)\n",
        "    beta = self.softmax(fc_layer)\n",
        "    gamma = self.dense(fc_layer)\n",
        "    \n",
        "    theta = (gamma, beta, delta)\n",
        "    #print('theta', theta)\n",
        "    #print('delta', delta)\n",
        "    #print('beta', beta)\n",
        "    \n",
        "    return theta, hidden \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKoU7fYqvbLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0005\n",
        "#learning_rate_decay = \n",
        "num_epochs = 15\n",
        "T = 3*64\n",
        "num_paths = 50\n",
        "pred_length = 64\n",
        "num_lstm_layers = 2\n",
        "hidden_units = 80\n",
        "batch_size = 10\n",
        "\n",
        "training_data = split_data(int(T/64), train_ratios[0])\n",
        "training_data.shape\n",
        "\n",
        "small_data = np.zeros((10, training_data.shape[1]))\n",
        "for i in range(small_data.shape[0]): \n",
        "  index = np.random.randint(0, training_data.shape[0])\n",
        "  small_data[i,:] = training_data[index]\n",
        "  \n",
        "small_data_new = torch.FloatTensor(small_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV2CJzS-EWKe",
        "colab_type": "code",
        "outputId": "9408e073-6253-4db7-93b3-23108ffb1b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "encoder = Encoder(1, hidden_units, batch_size, 1, num_lstm_layers)\n",
        "print(encoder)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (lstm): LSTM(1, 80, num_layers=2)\n",
            "  (linear): Linear(in_features=80, out_features=80, bias=True)\n",
            "  (dense): Linear(in_features=80, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIa10RoikYuz",
        "colab_type": "code",
        "outputId": "1c72ff7a-98bb-4182-e4c6-8344bbfa63ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "### TRAINING the encoder###\n",
        "\n",
        "##Question: should optimization be done on each time series or over loss for all TS?\n",
        "loss_function = crps_loss\n",
        "encoder_optimiser = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "train_loss = []\n",
        "\n",
        "for i in range(num_epochs): \n",
        "  encoder.zero_grad()\n",
        "  encoder_hidden = encoder.init_hidden()\n",
        "  \n",
        "  batch_loss = 0\n",
        "  \n",
        "  data_batch = small_data_new #change this when using larger dataset \n",
        "  \n",
        "  for j in range(batch_size):\n",
        "    loss_t = 0 \n",
        "    input_data = data_batch[j]\n",
        "    \n",
        "    #print('training batch element', j)\n",
        "    \n",
        "    for t in range(T-1): \n",
        "      theta, encoder_hidden = encoder(input_data[t:t+1], encoder_hidden)\n",
        "      loss_t += loss_function(theta, input_data[t+1:t+2])\n",
        " \n",
        "      \n",
        "    #ts_loss = loss_t  \n",
        "    #print(ts_loss)\n",
        "    \n",
        "    #encoder_optimiser.zero_grad()\n",
        "    #ts_loss.backward()\n",
        "    #encoder_optimiser.step()\n",
        "   \n",
        "    batch_loss+= loss_t\n",
        "    \n",
        "  loss_batch = batch_loss \n",
        "    \n",
        "  encoder_optimiser.zero_grad()\n",
        "  \n",
        "  batch_loss.backward()\n",
        "\n",
        "  encoder_optimiser.step()\n",
        "\n",
        "  train_loss.append(batch_loss.item())\n",
        "  \n",
        "  print(\"Epoch \", i, \"CRPS \", batch_loss.item())\n",
        "  "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 CRPS  1042.209716796875\n",
            "Epoch  1 CRPS  825.5733032226562\n",
            "Epoch  2 CRPS  636.6781616210938\n",
            "Epoch  3 CRPS  475.8078308105469\n",
            "Epoch  4 CRPS  343.047119140625\n",
            "Epoch  5 CRPS  238.12265014648438\n",
            "Epoch  6 CRPS  160.26931762695312\n",
            "Epoch  7 CRPS  108.00437927246094\n",
            "Epoch  8 CRPS  79.11420440673828\n",
            "Epoch  9 CRPS  73.12834167480469\n",
            "Epoch  10 CRPS  88.9939193725586\n",
            "Epoch  11 CRPS  118.69500732421875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-3461393c078c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mloss_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-6c5c3cab4c57>\u001b[0m in \u001b[0;36mcrps_loss\u001b[0;34m(theta, z)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-11b2049d0a62>\u001b[0m in \u001b[0;36msqf\u001b[0;34m(theta, quantile)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetbd_from_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmax_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-bdac65ad2bfc>\u001b[0m in \u001b[0;36mgetbd_from_theta\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9g08fYSzdts",
        "colab_type": "code",
        "outputId": "3d2c4eee-cca7-414f-f52e-e1973555f620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([889.9541], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouJ2LmtMwF6d",
        "colab_type": "code",
        "outputId": "4dc033f6-6e89-4e51-8ed7-2f3d7975a719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.plot(train_loss)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f683e13f7b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkA1XAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIPzPlCFq468",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PREDICTION ##\n",
        "decoder = Encoder()\n",
        "\n",
        "with torch.no_grad(): \n",
        "  for t in range(T): \n",
        "    theta, encoder_hidden = encoder(input_data[t], encoder_hidden)\n",
        "    \n",
        "  for n in range(num_paths): \n",
        "  \n",
        "    decoder_input = input_data[T]\n",
        "    decoder_hidden = encoder_hidden \n",
        "    alpha = torch.distributions.uniform.Uniform(0,1)\n",
        "    sample_path = []\n",
        "\n",
        "    for i in range(pred_length): \n",
        "      theta, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      z_hat = sqf(theta, alpha)\n",
        "      sample_path.append(z_hat)\n",
        "      decoder_input = z_hat\n",
        "      \n",
        "    path = input_data.append(sample_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}